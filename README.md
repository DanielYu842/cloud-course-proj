# Setup Guide

### Step 1: Environment Setup

**For Local Setup:**
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Download Data

```bash
pip install requests
python scripts/download_data.py
```

This will download 6 files total:
- NYC: listings.csv + calendar.csv
- LA: listings.csv + calendar.csv  
- Paris: listings.csv + calendar.csv


### Step 3: Run the Analysis

Follow the notebooks in order:
1. `01_data_ingestion.ipynb` - Load and explore data
2. `02_data_cleaning.ipynb` - Clean and preprocess
3. `03_feature_engineering.ipynb` - Create features
4. `04_eda_analysis.ipynb` - Exploratory analysis
6. `05_model_training.ipynb` - Build ML models
7. `06_performance_eval.ipynb` - Spark benchmarks

---

## ðŸ“‚ Project Structure Overview

```
cloud-project/
â”œâ”€â”€ README.md              # Main project documentation
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .gitignore             # Git ignore rules
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_data.py   # Automated data downloader
â”‚
â”œâ”€â”€ notebooks/             # Jupyter/Databricks notebooks
â”‚
â”œâ”€â”€ data/                  # Data directory (gitignored)
â”‚   â”œâ”€â”€ raw/              # Original CSV files
â”‚   â”‚   â”œâ”€â”€ nyc/
â”‚   â”‚   â”œâ”€â”€ la/
â”‚   â”‚   â””â”€â”€ paris/
â”‚   â””â”€â”€ processed/        # Cleaned datasets
â”‚
â”œâ”€â”€ outputs/              # Generated outputs
â”‚   â”œâ”€â”€ figures/         # Plots and charts
â”‚   â”œâ”€â”€ models/          # Saved ML models
â”‚   â””â”€â”€ results/         # Metrics and results

---
