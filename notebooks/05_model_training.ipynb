{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " starting ml pipeline\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import *\n",
        "import time\n",
        "print(f\" starting ml pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/13 01:28:27 WARN Utils: Your hostname, MacBook-Pro-110.local resolves to a loopback address: 127.0.0.1; using 10.2.1.42 instead (on interface en0)\n",
            "25/11/13 01:28:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/13 01:28:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " spark session connected\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/13 01:28:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AirbnbPricePredictor\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
        "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
        "    .getOrCreate()\n",
        "print(f\" spark session connected\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data loaded: 108,251 rows (outliers removed)\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.parquet(\"../data/processed/listings_features_no_outliers.parquet\")\n",
        "print(f\"data loaded: {df.count():,} rows (outliers removed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numeric features: 20\n",
            "categorical features: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/13 01:28:40 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "numeric_features = [\n",
        "    \"accommodates\", \"bedrooms\", \"beds\", \"bathrooms\",\n",
        "    \"minimum_nights\", \"latitude\", \"longitude\",\n",
        "    \"host_tenure_years\", \"host_response_rate\", \"host_listings_count\",\n",
        "    \"number_of_reviews\", \"reviews_per_month\", \"review_scores_rating\",\n",
        "    \"availability_365\", \"occupancy_rate\",\n",
        "    \"distance_to_center\", \"people_per_bedroom\",\n",
        "    \"host_performance_score\", \"popularity_score\",\n",
        "    \"neighborhood_listing_count\"\n",
        "]\n",
        "categorical_features = [\n",
        "    \"room_type\", \"property_category\", \"min_nights_category\",\n",
        "    \"city\", \"host_is_superhost\", \"instant_bookable\"\n",
        "]\n",
        "target = \"price\"\n",
        "numeric_features = [f for f in numeric_features if f in df.columns]\n",
        "categorical_features = [f for f in categorical_features if f in df.columns]\n",
        "print(f\"numeric features: {len(numeric_features)}\")\n",
        "print(f\"categorical features: {len(categorical_features)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Encode Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " categorical features encoded\n"
          ]
        }
      ],
      "source": [
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\")\n",
        "    for col in categorical_features\n",
        "]\n",
        "indexer_pipeline = Pipeline(stages=indexers)\n",
        "df_indexed = indexer_pipeline.fit(df).transform(df)\n",
        "indexed_features = [f\"{col}_index\" for col in categorical_features]\n",
        "print(f\" categorical features encoded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Assemble Feature Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/13 01:28:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data after removing nulls: 106,822 rows\n",
            " feature vector assembled\n"
          ]
        }
      ],
      "source": [
        "all_features = numeric_features + indexed_features\n",
        "df_ml = df_indexed.select(all_features + [target]).dropna()\n",
        "print(f\"data after removing nulls: {df_ml.count():,} rows\")\n",
        "assembler = VectorAssembler(inputCols=all_features, outputCol=\"features\")\n",
        "df_ml = assembler.transform(df_ml)\n",
        "df_ml = df_ml.select(\"features\", target)\n",
        "print(f\" feature vector assembled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set: 85,444 rows\n",
            "test set: 21,378 rows\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "train_data.cache()\n",
        "test_data.cache()\n",
        "print(f\"training set: {train_data.count():,} rows\")\n",
        "print(f\"test set: {test_data.count():,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model 1: Linear Regression (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "model 1: linear regression\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/13 01:28:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
            "25/11/13 01:28:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
            "25/11/13 01:28:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "training time: 0.55 seconds\n",
            "\n",
            "**linear regression results:**\n",
            "rmse: $79.59\n",
            "mae:  $59.19\n",
            "r²:   0.3697\n",
            "\n",
            "sample predictions:\n",
            "+-----+------------------+\n",
            "|price|prediction        |\n",
            "+-----+------------------+\n",
            "|300.0|157.09520447410102|\n",
            "|150.0|159.83930852729952|\n",
            "|160.0|152.09799452927155|\n",
            "|112.0|149.75648039858066|\n",
            "|112.0|162.73515810436692|\n",
            "|410.0|200.1114091229424 |\n",
            "|221.0|203.17362248674272|\n",
            "|101.0|194.6090694044994 |\n",
            "|129.0|157.13931191091342|\n",
            "|52.0 |148.5594811683281 |\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"=\" * 70)\n",
        "print(f\"model 1: linear regression\")\n",
        "print(f\"=\" * 70)\n",
        "start_time = time.time()\n",
        "lr = LinearRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=target,\n",
        "    maxIter=10,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0\n",
        ")\n",
        "lr_model = lr.fit(train_data)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\ntraining time: {training_time:.2f} seconds\")\n",
        "lr_predictions = lr_model.transform(test_data)\n",
        "evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
        "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
        "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
        "print(f\"\\n**linear regression results:**\")\n",
        "print(f\"rmse: ${lr_rmse:.2f}\")\n",
        "print(f\"mae:  ${lr_mae:.2f}\")\n",
        "print(f\"r²:   {lr_r2:.4f}\")\n",
        "print(f\"\\nsample predictions:\")\n",
        "lr_predictions.select(target, \"prediction\").limit(10).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model 2: Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "model 2: random forest\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/13 01:29:03 WARN DAGScheduler: Broadcasting large task binary with size 1202.7 KiB\n",
            "25/11/13 01:29:04 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
            "25/11/13 01:29:05 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
            "25/11/13 01:29:05 WARN DAGScheduler: Broadcasting large task binary with size 1042.6 KiB\n",
            "25/11/13 01:29:06 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n",
            "25/11/13 01:29:07 WARN DAGScheduler: Broadcasting large task binary with size 1812.1 KiB\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "training time: 7.27 seconds\n",
            "\n",
            "**random forest results:**\n",
            "rmse: $64.89\n",
            "mae:  $46.71\n",
            "r²:   0.5810\n",
            "\n",
            "top 15 most important features:\n",
            "+-------------------------+--------------------+\n",
            "|feature                  |importance          |\n",
            "+-------------------------+--------------------+\n",
            "|bedrooms                 |0.21570916998974685 |\n",
            "|accommodates             |0.13245132332367032 |\n",
            "|property_category_index  |0.0934478769494848  |\n",
            "|bathrooms                |0.08968153862584256 |\n",
            "|beds                     |0.05237194341597224 |\n",
            "|distance_to_center       |0.05194085262048233 |\n",
            "|room_type_index          |0.04113169315974795 |\n",
            "|longitude                |0.03753677024170569 |\n",
            "|city_index               |0.03416958627979777 |\n",
            "|host_listings_count      |0.030582216863634226|\n",
            "|people_per_bedroom       |0.02962633133832439 |\n",
            "|latitude                 |0.02646326633373878 |\n",
            "|minimum_nights           |0.026065539737205445|\n",
            "|min_nights_category_index|0.02224698174109056 |\n",
            "|review_scores_rating     |0.021556674438094878|\n",
            "+-------------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"=\" * 70)\n",
        "print(f\"model 2: random forest\")\n",
        "print(f\"=\" * 70)\n",
        "start_time = time.time()\n",
        "rf = RandomForestRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=target,\n",
        "    numTrees=50,\n",
        "    maxDepth=10,\n",
        "    seed=42\n",
        ")\n",
        "rf_model = rf.fit(train_data)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\ntraining time: {training_time:.2f} seconds\")\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
        "rf_mae = evaluator_mae.evaluate(rf_predictions)\n",
        "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
        "print(f\"\\n**random forest results:**\")\n",
        "print(f\"rmse: ${rf_rmse:.2f}\")\n",
        "print(f\"mae:  ${rf_mae:.2f}\")\n",
        "print(f\"r²:   {rf_r2:.4f}\")\n",
        "feature_importance = rf_model.featureImportances\n",
        "importance_list = [(all_features[i], float(feature_importance[i]))\n",
        "                   for i in range(len(all_features))]\n",
        "importance_df = spark.createDataFrame(importance_list, [\"feature\", \"importance\"]) \\\n",
        "    .orderBy(desc(\"importance\"))\n",
        "print(f\"\\ntop 15 most important features:\")\n",
        "importance_df.limit(15).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model 3: Gradient Boosted Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "model 3: gradient boosted trees\n",
            "======================================================================\n",
            "\n",
            "training time: 7.78 seconds\n",
            "\n",
            "**gradient boosted trees results:**\n",
            "rmse: $61.86\n",
            "mae:  $43.66\n",
            "r²:   0.6193\n",
            "\n",
            "top 15 most important features (gbt):\n",
            "+--------------------------+--------------------+\n",
            "|feature                   |importance          |\n",
            "+--------------------------+--------------------+\n",
            "|bedrooms                  |0.11206675382719887 |\n",
            "|distance_to_center        |0.10470007359276688 |\n",
            "|longitude                 |0.10151934450171603 |\n",
            "|latitude                  |0.09488215214704386 |\n",
            "|minimum_nights            |0.07246659377456735 |\n",
            "|review_scores_rating      |0.0658107763066241  |\n",
            "|accommodates              |0.054752281837974426|\n",
            "|host_response_rate        |0.05178471551622767 |\n",
            "|host_listings_count       |0.04890012784792459 |\n",
            "|city_index                |0.04498899071532918 |\n",
            "|property_category_index   |0.04284766763384124 |\n",
            "|people_per_bedroom        |0.027700732064641985|\n",
            "|reviews_per_month         |0.026898818377831295|\n",
            "|neighborhood_listing_count|0.026897911130001638|\n",
            "|bathrooms                 |0.025707737823472295|\n",
            "+--------------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"=\" * 70)\n",
        "print(f\"model 3: gradient boosted trees\")\n",
        "print(f\"=\" * 70)\n",
        "start_time = time.time()\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=target,\n",
        "    maxIter=50,\n",
        "    maxDepth=5,\n",
        "    seed=42\n",
        ")\n",
        "gbt_model = gbt.fit(train_data)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\ntraining time: {training_time:.2f} seconds\")\n",
        "gbt_predictions = gbt_model.transform(test_data)\n",
        "gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)\n",
        "gbt_mae = evaluator_mae.evaluate(gbt_predictions)\n",
        "gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
        "print(f\"\\n**gradient boosted trees results:**\")\n",
        "print(f\"rmse: ${gbt_rmse:.2f}\")\n",
        "print(f\"mae:  ${gbt_mae:.2f}\")\n",
        "print(f\"r²:   {gbt_r2:.4f}\")\n",
        "gbt_feature_importance = gbt_model.featureImportances\n",
        "gbt_importance_list = [(all_features[i], float(gbt_feature_importance[i]))\n",
        "                       for i in range(len(all_features))]\n",
        "gbt_importance_df = spark.createDataFrame(gbt_importance_list, [\"feature\", \"importance\"]) \\\n",
        "    .orderBy(desc(\"importance\"))\n",
        "print(f\"\\ntop 15 most important features (gbt):\")\n",
        "gbt_importance_df.limit(15).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "model comparison summary\n",
            "======================================================================\n",
            "\n",
            "\n",
            "                 Model  RMSE ($)   MAE ($)  R² Score\n",
            "     Linear Regression 79.586787 59.190476  0.369747\n",
            "         Random Forest 64.888396 46.714432  0.581045\n",
            "Gradient Boosted Trees 61.858598 43.656816  0.619256\n",
            "\n",
            " best model: Gradient Boosted Trees\n",
            "   r² score: 0.6193\n",
            "   rmse: $61.86\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "print(f\"=\" * 70)\n",
        "print(f\"model comparison summary\")\n",
        "print(f\"=\" * 70)\n",
        "comparison = pd.DataFrame({\n",
        "    \"Model\": [\"Linear Regression\", \"Random Forest\", \"Gradient Boosted Trees\"],\n",
        "    \"RMSE ($)\": [lr_rmse, rf_rmse, gbt_rmse],\n",
        "    \"MAE ($)\": [lr_mae, rf_mae, gbt_mae],\n",
        "    \"R² Score\": [lr_r2, rf_r2, gbt_r2]\n",
        "})\n",
        "print(f\"\\n\")\n",
        "print(comparison.to_string(index=False))\n",
        "best_model_idx = comparison[\"R² Score\"].idxmax()\n",
        "best_model_name = comparison.loc[best_model_idx, \"Model\"]\n",
        "print(f\"\\n best model: {best_model_name}\")\n",
        "print(f\"   r² score: {comparison.loc[best_model_idx, 'R² Score']:.4f}\")\n",
        "print(f\"   rmse: ${comparison.loc[best_model_idx, 'RMSE ($)']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Prediction Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error statistics:\n",
            "+--------------------+-----------------+------------------+------------------+-----------------+\n",
            "|mean_error          |mean_abs_error   |mean_pct_error    |median_abs_error  |median_pct_error |\n",
            "+--------------------+-----------------+------------------+------------------+-----------------+\n",
            "|-0.26373672091718936|43.65681588637874|29.562648082475675|30.953707133511486|22.29909701791363|\n",
            "+--------------------+-----------------+------------------+------------------+-----------------+\n",
            "\n",
            "\n",
            "percentage of predictions within $50: 68.97%\n",
            "\n",
            "worst 10 predictions (highest error):\n",
            "+-----+------------------+-------------------+------------------+\n",
            "|price|prediction        |error              |abs_error         |\n",
            "+-----+------------------+-------------------+------------------+\n",
            "|550.0|32.17890297866269 |-517.8210970213373 |517.8210970213373 |\n",
            "|540.0|95.63878105557335 |-444.36121894442664|444.36121894442664|\n",
            "|500.0|83.54068709937494 |-416.45931290062504|416.45931290062504|\n",
            "|500.0|98.1294782456422  |-401.8705217543578 |401.8705217543578 |\n",
            "|495.0|101.24965441749205|-393.750345582508  |393.750345582508  |\n",
            "|500.0|120.19689059672778|-379.80310940327223|379.80310940327223|\n",
            "|459.0|83.71267054358049 |-375.2873294564195 |375.2873294564195 |\n",
            "|215.0|588.5206109401137 |373.52061094011367 |373.52061094011367|\n",
            "|500.0|139.0497759048773 |-360.95022409512273|360.95022409512273|\n",
            "|467.0|108.39150783533424|-358.6084921646658 |358.6084921646658 |\n",
            "+-----+------------------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "error_analysis = gbt_predictions.withColumn(\n",
        "    \"error\",\n",
        "    col(\"prediction\") - col(target)\n",
        ").withColumn(\n",
        "    \"abs_error\",\n",
        "    abs(col(\"error\"))\n",
        ").withColumn(\n",
        "    \"pct_error\",\n",
        "    (abs(col(\"error\")) / col(target)) * 100\n",
        ")\n",
        "print(f\"error statistics:\")\n",
        "error_analysis.select(\n",
        "    mean(\"error\").alias(\"mean_error\"),\n",
        "    mean(\"abs_error\").alias(\"mean_abs_error\"),\n",
        "    mean(\"pct_error\").alias(\"mean_pct_error\"),\n",
        "    expr(\"percentile(abs_error, 0.5)\").alias(\"median_abs_error\"),\n",
        "    expr(\"percentile(pct_error, 0.5)\").alias(\"median_pct_error\")\n",
        ").show(truncate=False)\n",
        "within_50_count = error_analysis.filter(col(\"abs_error\") <= 50).count()\n",
        "total_count = error_analysis.count()\n",
        "percent_within_50 = (within_50_count / total_count) * 100 if total_count > 0 else 0.0\n",
        "print(f\"\\npercentage of predictions within $50: {percent_within_50:.2f}%\")\n",
        "print(f\"\\nworst 10 predictions (highest error):\")\n",
        "error_analysis.select(target, \"prediction\", \"error\", \"abs_error\") \\\n",
        "    .orderBy(desc(\"abs_error\")) \\\n",
        "    .limit(10) \\\n",
        "    .show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " model saved to: ../outputs/models/gbt_model\n",
            "\n",
            " model training complete\n"
          ]
        }
      ],
      "source": [
        "model_path = \"../outputs/models/gbt_model\"\n",
        "try:\n",
        "    gbt_model.write().overwrite().save(model_path)\n",
        "    print(f\" model saved to: {model_path}\")\n",
        "except:\n",
        "    print(f\" could not save model to dbfs (permission issue)\")\n",
        "    print(f\"  model is still available in memory for this session\")\n",
        "metrics = {\n",
        "    \"model_type\": \"RandomForest\",\n",
        "    \"rmse\": float(gbt_rmse),\n",
        "    \"mae\": float(gbt_mae),\n",
        "    \"r2\": float(gbt_r2),\n",
        "    \"num_trees\": 50,\n",
        "    \"max_depth\": 10\n",
        "}\n",
        "print(f\"\\n model training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
